{
  
    
        "post0": {
            "title": "Testing the match case statements",
            "content": "In this tutorial, we&#39;ll take a look at the new match-case statement released in Python 3.10. . We&#39;ll look at: . What is the match-case statement? | What features does it have? | Why would you use match-case over an if-else statement? | 1. What is the match-case statement? . On the surface it looks very simliar to an if-else statement. You provide some conditions, and the statement retuns a value if those conditions are met. For example, here we have a match-case to match the London TFL tube to that line&#39;s official colour. . def find_tube(tube_line_colour): match tube_line_colour: case &quot;black&quot;: return &quot;northern&quot; case &quot;red&quot; | &quot;orange&quot;: return &quot;central&quot; case _: return &quot;unknown&quot; . display(find_tube(&quot;red&quot;)) display(find_tube(&quot;darkgreen&quot;)) . &#39;central&#39; . &#39;unknown&#39; . We could also implement the same logic using an if-else statement: . def find_tube(tube_line_colour): if tube_line_colour == &quot;black&quot;: return &quot;northern&quot; elif tube_line_colour == &quot;red&quot; or tube_line_colour == &quot;orange&quot;: return &quot;central&quot; else: return &quot;unknown&quot; . display(find_tube(&quot;red&quot;)) display(find_tube(&quot;darkgreen&quot;)) . &#39;central&#39; . &#39;unknown&#39; . 2. What features does it have? .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2022/06/27/match-case.html",
            "relUrl": "/2022/06/27/match-case.html",
            "date": " • Jun 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Measuring Equity Risk",
            "content": "In this notebook we&#39;ll explore two statistical methods for calculating Equity Risk: . Variance: fluctuation of stock return from its mean | VaR: the maximum an investor could loose (within a confidence interval) | For the purposes of this notebook, we&#39;ll explore the above looking at the risk of Apple (ticker = AAPL). . Import packages and setup configurations . import requests import json import datetime import os # Import data wrangling packages import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # Use Python&#39;s &quot;magic&quot; commands since we want to see the graphs within this notebook %matplotlib inline %pylab inline # Plot configuration plt.style.use(&#39;fivethirtyeight&#39;) pylab.rcParams[&quot;figure.figsize&quot;] = (10,8) . Populating the interactive namespace from numpy and matplotlib . API_KEY = os.getenv(&quot;AV_API_KEY&quot;) . Collect prices from the stock price API . def get_adj_prices(symbol): url = f&quot;https://www.alphavantage.co/query?function=TIME_SERIES_WEEKLY_ADJUSTED&amp;symbol={symbol}&amp;apikey={API_KEY}&quot; raw_data = requests.get(url).text json_data = json.loads(raw_data)[&quot;Weekly Adjusted Time Series&quot;] return json_data . Build a dataframe from the stock prices . def build_stock_df(symbol): json_data = get_adj_prices(symbol) df = pd.DataFrame.from_dict(json_data).transpose() adj_col = &quot;5. adjusted close&quot; df = df[[adj_col]].rename(columns={adj_col: f&quot;price_{symbol}&quot;}) return df . aapl = build_stock_df(&quot;aapl&quot;).head(1000) spy = build_stock_df(&quot;spy&quot;).head(1000) ts_df = aapl.join(spy).astype(float) . ts_df[&quot;aapl_returns&quot;] = ts_df[&quot;price_aapl&quot;].pct_change(1) ts_df[&quot;spy_returns&quot;] = ts_df[&quot;price_spy&quot;].pct_change(1) . ts_df.head(5) . price_aapl price_spy aapl_returns spy_returns . 2022-09-14 155.31 | 394.60 | NaN | NaN | . 2022-09-09 157.37 | 406.60 | 0.013264 | 0.030411 | . 2022-09-02 155.81 | 392.24 | -0.009913 | -0.035317 | . 2022-08-26 163.62 | 405.31 | 0.050125 | 0.033321 | . 2022-08-19 171.52 | 422.14 | 0.048283 | 0.041524 | . Calculate stock variance . First, lets check the summary statistics for both AAPL and the S&amp;P Index. We can also plot the returns as a histogram to get an idea of dispersion of returns. . # The minimum return was -6.5% and the max 8.2% aapl_summary = ts_df[&quot;aapl_returns&quot;].describe() aapl_summary . count 999.000000 mean -0.005106 std 0.045935 min -0.165550 25% -0.032927 50% -0.007687 75% 0.020599 max 0.321114 Name: aapl_returns, dtype: float64 . ts_df[&quot;aapl_returns&quot;].hist(bins=50, figsize=(8,6)) . &lt;AxesSubplot:&gt; . # Lets check on what date AAPL had a min and max return max_date = ts_df.loc[ts_df[&#39;aapl_returns&#39;].idxmax()].name min_date = ts_df.loc[ts_df[&#39;aapl_returns&#39;].idxmin()].name print (&quot;The max value for aapl was recorded on {} while the min value was recorded on {}&quot;.format(max_date, min_date)) . The max value for aapl was recorded on 2008-09-26 while the min value was recorded on 2006-07-14 . snp_summary = ts_df[&quot;spy_returns&quot;].describe() snp_summary . count 999.000000 mean -0.001434 std 0.024947 min -0.117327 25% -0.014145 50% -0.003155 75% 0.008836 max 0.246780 Name: spy_returns, dtype: float64 . Plot the returns distributions . style.use(&#39;fivethirtyeight&#39;) x = ts_df[&quot;spy_returns&quot;] * 100 y = ts_df[&quot;aapl_returns&quot;] * 100 plt.hist(x, alpha = 0.5, bins=50, label=&#39;S&amp;P 500&#39;) plt.hist(y,alpha = 0.5, bins = 50, label=&#39;Apple Inc.&#39;) plt.legend(loc=&#39;upper right&#39;) plt.tick_params(axis = &#39;both&#39;, which = &#39;major&#39;, labelsize = 9) plt.axhline(y = 0, color = &#39;black&#39;, linewidth = 5, alpha = 1) #plt.xlabel(s = &quot;% return&quot;, fontsize = &quot;small&quot;) plt.text(-15, 180, s = &quot;&quot;&quot;This grap plots the distribution of weeklyreturns of Apple Inc. against returns of the S&amp;P 500 between 2003-07-25 and today. The histogram for Apple Inc. is more dispersed, with more extreme highs and lows. During this period, Apple Inc. had a high of 32% and a low of -16%.&quot;&quot;&quot;, fontsize = 15, alpha = .75) plt.text(-15, 215, s = &quot;&quot;&quot;Plotting return distributions&quot;&quot;&quot;, weight = &quot;bold&quot;, fontsize = 25, alpha = .75) plt.show() . . Value at Risk (VaR) . In this section we&#39;ll check the VaR numbers for both AAPL and the S&amp;P500. The VaR tell us that we have X % chance of not loosing more than a nominal value in one day. . def var_cov_var(P, c, m, sigma): &quot;&quot;&quot; Variance-Covariance calculation of weekly Value-at-Risk using confidence level c, with mean of returns m and standard deviation of returns sigma, on a portfolio of value P. &quot;&quot;&quot; alpha = norm.ppf(1-c, m, sigma) return P - P*(alpha + 1) . . from scipy.stats import norm P = 1000000 # 1,000,000 USD c = 0.99 # 99% confidence interval aapl_mean = ts_df[&quot;aapl_returns&quot;].mean() aapl_sigma = ts_df[&quot;aapl_returns&quot;].std() snp_mean = ts_df[&quot;spy_returns&quot;].mean() snp_sigma = ts_df[&quot;spy_returns&quot;].std() var_aapl = var_cov_var(P, c, aapl_mean, aapl_sigma) var_snp = var_cov_var(P, c, snp_mean, snp_sigma) print (&quot;The one week VaR for Apple on a $1 million portfolio is ${:,.2f}&quot;.format(round(var_aapl,2))) print (&quot;The one week VaR for the S&amp;P 500 on a $1 million portfolio is ${:,.2f}&quot;.format(round(var_snp,2))) . The one week VaR for Apple on a $1 million portfolio is $111,966.53 The one week VaR for the S&amp;P 500 on a $1 million portfolio is $59,469.59 .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/03/03/measure-equity-risk.html",
            "relUrl": "/2018/03/03/measure-equity-risk.html",
            "date": " • Mar 3, 2018"
        }
        
    
  
    
        ,"post2": {
            "title": "Find all Tuesdays",
            "content": "In this tutorial we look at two ways to solve the following problem: . Count the number of Tuesday between 1st of January 2000 and 31st of December 2020 . First, we find the answer using the very helpful Datetime package. This package does most of the heavy lifting for us. Then, for a bit of fun, we try and find the same solution using Python&#39;s default data types only. . 1. With the datetime package . Start by importing the datetime and timedelta class. . from datetime import datetime, timedelta . Then write a function which takes two paraters, start_year and end_year, which returns the number of Tuesdays between these years. There is an assumption that start_year starts on 1st day of the year, and end_year ends on the last day of that year. . def count_tuesdays(start_year, end_year): &quot;&quot;&quot; Returns the number of Tuesaday between two dates. Parameters: start_year (int): The starting year end_year (int): The end year Returns: count_tuesdays(int): The number of Tuesdays between start_year and end_year &quot;&quot;&quot; # 1. Find the number of days between start and end year datetime_range = datetime(year=end_year, month=1, day=1) - datetime(year=start_year, month=12, day=31) # 2. Build a list of Datetimes for this range date_range = [datetime(year=start_year, month=1, day=1) + timedelta(days=x) for x in range(datetime_range.days)] # 3. Count the number of Tuesdays in this range count_tuesdays = len([i for i in date_range if i.isoweekday() == 2]) return count_tuesdays . count_tuesdays(2000, 2021) . 1044 . 2. Using default Python data structures . Next, lets make it a little harder to solve. Lets find the same result, this time only using Python&#39;s default data structures. In order words, we cannot rely on the datetime package. . First, we create a simple Date object which has the following attributes: . year | month | day | is_leap | day_of_week | day_of_weel_decode | There is nothing too complicated here. We assign some validation on the attributes. For example, the days cannot be greater than 31 and months cannot be greater than 12. . class Date(): def __init__(self, year, month, day, day_of_week): self.year = year self.is_leap = self._is_leap(self.year) self.month = month self.day = day self.day_of_week = day_of_week self.day_of_week_decode = self._day_of_week_decode() @property def day(self): return self._day @day.setter def day(self, value): if (value &gt; 31): raise Exception(&quot;Days cannot be greater than 31&quot;) if (self.month in [9, 4, 6, 11] and value &gt; 31): raise Exception(f&quot;Days cannot be greater than 30 for month {self.month}&quot;) if (self.month == 2): if (self.is_leap == True and value &gt; 29): raise Exception(f&quot;Days must be 29 or less for a leap year Febuary&quot;) if (self.is_leap == False and value &gt; 28): raise Exception(f&quot;Days must be 28 or less for a non-leap year Febuary&quot;) self._day = value @property def month(self): return self._month @month.setter def month(self, value): if (value &gt; 12): raise Exception(&quot;Month cannot be greater than 12&quot;) self._month = value @property def year(self): return self._year @year.setter def year(self, value): self._year = value @property def day_of_week(self): return self._day_of_week @day_of_week.setter def day_of_week(self, value): self._day_of_week = value def _is_leap(self, year): if (year % 4 == 0) &amp; (year % 100 != 0): return True if (year % 100 == 0) &amp; (year % 400 == 0): return True return False def _day_of_week_decode(self): decodes = { 1: &quot;Mon&quot;, 2: &quot;Tue&quot;, 3: &quot;Wed&quot;, 4: &quot;Thu&quot;, 5: &quot;Fri&quot;, 6: &quot;Sat&quot;, 7: &quot;Sun&quot; } return decodes[self.day_of_week] def __str__(self): return f&quot;{self.year}-{self.month}-{self.day}&quot; def __repr__(self): return f&quot;Date({self.year}-{self.month}-{self.day}, {self.day_of_week_decode})&quot; . Let&#39;s build a sample date object to check everything looks ok. We&#39;ll create a Date for Tuesday 29th of Febuary 2000. . Date(2000, 2, 29, 2) . Date(2000-2-29, Tue) . try: sample_date = Date(2001, 2, 29, 2) except Exception as e: print(e) . Days must be 28 or less for a non-leap year Febuary . Next, let&#39;s build up a range of dates. First we need a generator which will return a weekday label (1 to 7) for each date in our range. We know that the 1st of January is a saturday so we will build our iterable from that point. . class DateRange(): def __init__(self, start_year, end_year, start_day_of_week): self.start_year = start_year self.end_year = end_year self.start_day_of_week = start_day_of_week self.range = self.build_range(start_year, end_year, start_day_of_week) def _days_of_week(self, starting_day): days_of_week = list(range(1,8)) days_of_week = days_of_week[(starting_day-1):] + days_of_week[:(starting_day-1)] while True: for n in days_of_week: yield(n) def _is_leap(self, year): if (year % 4 == 0) &amp; (year % 100 != 0): return True if (year % 100 == 0) &amp; (year % 400 == 0): return True return False def build_range(self, start_year, end_year, start_day_of_week): day_of_week_iterator = self._days_of_week(start_day_of_week) date_range = [] for year in range(start_year, end_year+1): for month in range(1, 13): if month in [9, 4, 6, 11]: day_count = 30 elif (month == 2 and self._is_leap(year)): day_count = 29 elif (month == 2 and self._is_leap(year) is False): day_count = 28 else: day_count = 31 days_in_month = [Date(year, month, i, next(day_of_week_iterator)) for i in range(1, day_count+1)] date_range.extend(days_in_month) return date_range . date_range = DateRange(2000, 2019, 6).range count_tuesdays = len([i for i in date_range if i.day_of_week == 2]) count_tuesdays . 1044 .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/03/01/count-tuesdays.html",
            "relUrl": "/2018/03/01/count-tuesdays.html",
            "date": " • Mar 1, 2018"
        }
        
    
  
    
        ,"post3": {
            "title": "Copying objects in Python",
            "content": "In this tutorial we compare a shallow versus deep copy in Python using the inbuilt copy module. . from copy import copy . 1. Assignment . First we create a list called foo_list which has three items: two ints and one list of ints. We run the in-built id function to check the address of the list object in memory. . foo_list = [1, 2, [3, 4]] # check the item&#39;s address in memory id(foo_list) . 2930710693248 . Next we create a second variable called foo_list_two which is assigned to foo_list. We can see that both variables point to the same object in memory. . foo_list_two = foo_list # Check the two variables point to the same object id(foo_list) == id(foo_list_two) . True . foo_list[0] = 11 # Check the second list foo_list_two . [11, 2, [3, 4]] . %reset -f . 3. Shallow copy . foo_list = [1, 2, [3, 4]] # check the item&#39;s address in memory id(foo_list) . 2930709728704 . foo_list_two = foo_list.copy() # Check the two variables point to the same object id(foo_list) == id(foo_list_two) . False . foo_list[2][0] = 11 # Check the second list foo_list_two . [1, 2, [11, 4]] . 3. Deep copy . A deep copy creates a new object and recursively adds the copies of nested objects present in the original elements. . from copy import deepcopy . foo_list = [1, 2, [3, 4]] # check the item&#39;s address in memory id(foo_list) . 2930709811328 . foo_list_two = deepcopy(foo_list) # Check the two variables point to different objects id(foo_list) == id(foo_list_two) . False . foo_list[2][0] = 11 # Check that the deep copied list does not update print(foo_list) print(foo_list_two) . [11, 2, [11, 4]] [1, 2, [3, 4]] . foo_list[2][0] = 22 . print(foo_list) print(foo_list_two) . [11, 2, [22, 4]] [1, 2, [3, 4]] .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/01/20/copying-objects.html",
            "relUrl": "/2018/01/20/copying-objects.html",
            "date": " • Jan 20, 2018"
        }
        
    
  
    
        ,"post4": {
            "title": "Deploying a Python app - Part 2 (Kubernetes)",
            "content": "In Part 1 of this tutorial, we deployed a simple Flask app using docker. In this part, we&#39;ll deploy that same app using Kubernetes. . Create Kubernetes configuration . For the Kubernetes deployment, we&#39;ll need to create a deployment for managing pods/containers. . We create the deployment using a YAML file. . trains-deployment.yaml . apiVersion: apps/v1 kind: Deployment metadata: name: trains spec: selector: matchLabels: app: trains replicas: 2 template: metadata: labels: app: trains spec: containers: - name: trains image: slemasne/trains:latest resources: limits: memory: &quot;100Mi&quot; requests: memory: &quot;50Mi&quot; ports: - containerPort: 5000 . Check that the pods are running . We can see that two pods are running using the following command: . $ kubectl get pods NAME READY STATUS RESTARTS AGE trains-758d4f498c-kbs9k 1/1 Running 0 6m19s trains-758d4f498c-kfbpq 1/1 Running 1 6m19s . Run port forward . Next we run a port-forward command so we can connect to the cluster from our local machine. . kubectl port-forward deployment/trains 5000:5000 . We check the pod is online by running a curl command. . $ curl -I http://127.0.0.1:5000/ . This command returns a status code of 200: . HTTP/1.0 200 OK Content-Type: text/html; charset=utf-8 Content-Length: 21 Server: Werkzeug/1.0.1 Python/3.7.9 Date: Wed, 14 Sep 2022 18:01:56 GMT .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/01/04/flask-app-with-k8s.html",
            "relUrl": "/2018/01/04/flask-app-with-k8s.html",
            "date": " • Jan 4, 2018"
        }
        
    
  
    
        ,"post5": {
            "title": "Deploying a Python app - Part 1 (Docker)",
            "content": "In this tutorial, we show how you can build and deploy a really simple Python app using Docker. To demonstrate, we&#39;ll create an API to return details of the TFL train lines in London. We&#39;ll use Flask to build the application. . Create a Flask app . First, create a python flask app which returns details about the TFL trains. This application serves a very basic API with details on London&#39;s TFL trains. . from flask import Flask, jsonify, request import sys import logging logging.basicConfig(level=logging.INFO) app = Flask(__name__) tasks = [ { &#39;id&#39;: 1, &#39;service&#39;: &#39;tube&#39;, &#39;line&#39;: &#39;northern&#39;, &#39;colour&#39;: &#39;black&#39; }, { &#39;id&#39;: 2, &#39;service&#39;: &#39;tube&#39;, &#39;line&#39;: &#39;circle&#39;, &#39;colour&#39;: &#39;red&#39; } ] def shutdown_server(): func = request.environ.get(&#39;werkzeug.server.shutdown&#39;) if func is None: raise RuntimeError(&#39;Not running with the Werkzeug Server&#39;) func() @app.route(&#39;/trains&#39;, methods=[&#39;GET&#39;]) def get_tasks(): return jsonify({&#39;tasks&#39;: tasks}) @app.route(&#39;/&#39;) def hello_world(): return &#39;Welcome to trains API&#39; @app.route(&#39;/exit&#39;) def exit(): message = logging.info(&quot;Stopping application&quot;) shutdown_server() print(&quot;The Flask server has been shutdown.&quot;) if __name__ == &#39;__main__&#39;: app.run(debug=True, host=&#39;0.0.0.0&#39;) . . Run a quick &#39;hello world&#39; type test . import requests url = &quot;http://192.168.1.74:5000/&quot; response = requests.get(url) response_code = response.status_code response_text = requests.get(url).text display(response_code) display(response_text) . 200 . &#39;Welcome to trains API&#39; . Setup the Dockerfile . Next, we want to package the application into a container using a Dockerfile. The finished Dockerfile will look as follows: . FROM python:3.7-alpine COPY . /app WORKDIR /app RUN pip install -r requirements.txt EXPOSE 5000 CMD [&quot;python&quot;, &quot;app.py&quot;] . Deconstruct the Dockerfile . Let&#39;s break down each line in turn to describe what is happening... . In the first line we declare a parent image which is the image our own image is based on. Each subsequent declaration in the Dockerfile modifies this image. In our example, we use a version of Alpine Linux as our base image. Alpine Linux is a lightweight Linux distribution which makes it ideal for our container. . FROM python:3.7-alpine . Next we copy all of the files from the current host directory into the container&#39;s app directory. . COPY . /app . We change our working directory to the app directory. And then we tell Docker to install the Python packages needed for the app.py . WORKDIR /app RUN pip install -r requirements.txt . Next we tell the container to listen on a specific network port at runtime. The default is TCP but you can also specify UDP. Note that the EXPOSE instruction does not actually publish the port. This instruction is there for documentation purposes. To actually publish the port, you need to use the -p flag on the docker run command. . EXPOSE 5000 . Finally, we run the app: . CMD [&quot;python&quot;, &quot;app.py&quot;] . Run the docker container locally . You might want to run the Dockerfile on your own machine to verify that it is working correctly. To do that, we docker build and then docker run: . You can build the Dockerfile as follows. The -t or tag allows you to tag the image with a name. . docker build -t slemasne/trains . . Then run the image to create a container which runs our application. The &#39;-p&#39; flag maps port 5000 on localhost in the host to port 5000 in the docker container. . docker run -p 5000:5000 slemasne/trains . You can also run the command with a &#39;-d&#39; detached flag to run the container in the background: . docker run -d -p 5000:5000 slemasne/trains . The application can be accessed on our localhost: . http://localhost:5000/trains .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/01/03/flask-app-with-docker.html",
            "relUrl": "/2018/01/03/flask-app-with-docker.html",
            "date": " • Jan 3, 2018"
        }
        
    
  
    
        ,"post6": {
            "title": "SVP100 - Hitting The Wall",
            "content": "In August of 2017 I ran the SVP100. This is a 100KM trail running race starting in Newmarket (Suffolk, UK) and ending in Manning Tree (Essex, UK). In regular distance marathons, its widely believed runners &quot;hit the wall&quot; at around 30KM mark (or 70% through the race). In this notebook, I want to see if the same holds true over a 100KM distance. . From a high level, the code below performs the following: . Scrape results data from the race website (using Beautiful Soup) | Render scraped data into a DataFrame (using Pandas) | Format (or wrangle) the data into formats we can work with | Present results in time series graph (using Seaborn) | . 1. Import packages and set configurations . First, let&#39;s import some packages. We&#39;ll use BeautifulSoup for web scraping, pandas for data analysis, and then seaborn for plotting. . import datetime import time import requests from bs4 import BeautifulSoup import pandas as pd import seaborn as sns import matplotlib.pyplot as plt . Then we set the plotting configuration. I really like the &quot;fivethirtyeight&quot; stylesheet which generates plots in the style used by fivethirtyeight.com. . %matplotlib inline %pylab inline # Set style to fivethirtyeight to create clean and clear looking graphs plt.style.use(&#39;fivethirtyeight&#39;) # Define a dictionary containing default plotting configurations params = {&#39;legend.fontsize&#39;: &#39;small&#39;, &#39;figure.figsize&#39;: (12, 4.5), &#39;axes.labelsize&#39;: &#39;small&#39;, &#39;axes.titlesize&#39;:&#39;medium&#39;, &#39;xtick.labelsize&#39;:&#39;small&#39;, &#39;ytick.labelsize&#39;:&#39;small&#39;} pylab.rcParams.update(params) . Populating the interactive namespace from numpy and matplotlib . Next define some constants that we&#39;ll use in the notebook. First we define a URL which has a table which contains race result data. Then we have some details about the race distance. . BASE_URL = &quot;http://www.svp100.co.uk/results-&quot; YEAR = 2017 # Constants of race details DISTANCE_BETWEEN_CPS = [19.312, 17.702, 16.094, 17.702, 9.657, 13.679, 7.242] CPS_IN_KM = cp_miles = [(i * 1.60934) for i in [12, 23, 33, 44, 50, 58.5, 63]] . 2. Generate unformatted table of race data . In the function below, we request data from the race website and return a DataFrame. For the most part, the DataFrame is unformatted. . def df_builder(base_url=BASE_URL, year=YEAR): &quot;&quot;&quot; This function returns a pandas DataFrame which contain data scraped from the race website. The data is unformatted. Attributes: -- base_url (str): the url which contains race data in HTML year (int): the year for we would like data &quot;&quot;&quot; # Scrape the data from the race website url = base_url + str(year) r = requests.get(url).text soup = BeautifulSoup(r, &#39;lxml&#39;) # Find tables from the html rows = soup.find_all(&#39;tr&#39;)[1:] # Collect and format column names for the dataframe column_html = soup.find_all(&#39;th&#39;)[:] columns = [i.contents[0].lower().replace(&quot;/&quot;,&quot;&quot;).replace(&quot; &quot;, &quot;_&quot;) for i in column_html if i.contents[0]] # Build a dataframe data = [] for line in rows: row = line.find_all(&#39;td&#39;) row_list = [] for counter, value in enumerate(row): row_list.append(row[counter].string) data.append(row_list) # Remove columns we don&#39;t need df = pd.DataFrame(data, columns=columns).drop(columns = [&quot;name&quot;, &quot;club&quot;, &quot;mf&quot;, &quot;bib&quot;, &quot;total_time&quot;]).set_index(&quot;position&quot;) return df . . Generate a DataFrame of race results for the year 2017. . unformatted_df = df_builder(BASE_URL, YEAR) unformatted_df.head(2) . start cp1 cp2 cp3 cp4 cp5 cp6 finish . position . 1 08:30:00 | 09:58:30 | 11:26:51 | 12:58:00 | 14:34:00 | 15:41:00 | 17:04:49 | 17:54:10 | . 2 08:30:00 | 09:56:03 | 11:22:10 | 13:20:00 | 15:14:00 | 16:31:00 | 18:13:51 | 19:09:10 | . Scanning the data, we notice a few things... . The &quot;time&quot; related columns are strings representing a time of day. We&#39;ll need to convert these to datetimes so we can calculate the time (in seconds) between each checkpoint. . | There are some None values which will need to be removed. . | 3. Calculate the time for each runner between checkpoints . In the function below, we calculate the time (in seconds) it took each runner to run between checkpoints. Before we start, however, we want to remove any rows which have None values. . unformatted_df = unformatted_df.dropna().copy() . Now onto our function... . def calculate_time_to_checkpoints(df): &quot;&quot;&quot; This function returns a pandas DataFrame which contains time in seconds it took each runner to reach a checkpoint Attributes: -- df (DataFrame): a dataframe of race results &quot;&quot;&quot; # List out a set of columns which we&#39;ll need to convert into datimes dt_cols = [&quot;start&quot;, &quot;cp1&quot;, &quot;cp2&quot;, &quot;cp3&quot;, &quot;cp4&quot;, &quot;cp5&quot;, &quot;cp6&quot;, &quot;finish&quot;] # Convert these columns to datetimes for col in dt_cols: df[col] = df[col].apply(lambda x: pd.to_datetime(&quot;2017-08-08 &quot; + str(x))) # Then calculate the time in seconds between each checkpoint for start, end in zip(dt_cols, dt_cols[1:]): df[f&quot;time_to_{end}&quot;] = (df[end] - df[start]).apply(lambda x: x.seconds) # Finally lets drop the old &#39;time of day&#39; columns df = df.drop(columns=dt_cols).rename(columns={&quot;time_to_finish&quot;: &quot;time_to_cp7&quot;}).copy() return df . . df_time_to_cp = calculate_time_to_checkpoints(unformatted_df) df_time_to_cp.head(3) . time_to_cp1 time_to_cp2 time_to_cp3 time_to_cp4 time_to_cp5 time_to_cp6 time_to_cp7 . position . 1 5310 | 5301 | 5469 | 5760 | 4020 | 5029 | 2961 | . 2 5163 | 5167 | 7070 | 6840 | 4620 | 6171 | 3319 | . 3 6284 | 6382 | 6534 | 6720 | 4440 | 5869 | 3426 | . 4. Calculate the minutes per KM for each runner between checkpoints . Next we calculate the minutes per km it took each runner to move between checkpoints. . def calc_mins_per_km(df): &quot;&quot;&quot; This function returns a pandas DataFrame which contains min per km it took each runner to move between checkpoints Attributes: -- df (DataFrame): a dataframe of race results &quot;&quot;&quot; df = df.copy() for cp_time, distance in zip(df.columns, DISTANCE_BETWEEN_CPS): df[f&quot;min_per_km_{cp_time[-3:]}&quot;] = df[cp_time].apply(lambda x: (x / distance) / 60) df.drop(columns=[cp_time], inplace=True) df.columns = CPS_IN_KM return df . . The DataFrame below shows how long it took to move between each checkpoint. In this DataFrame we&#39;ve also replaced the checkpoint numbers with their distances in kilometers. . df_min_per_km = calc_mins_per_km(df_time_to_cp) df_min_per_km.head(3) . 19.31208 37.01482 53.10822 70.81096 80.46700 94.14639 101.38842 . position . 1 4.582643 | 4.990961 | 5.663601 | 5.423116 | 6.937972 | 6.127397 | 6.814416 | . 2 4.455779 | 4.864799 | 7.321569 | 6.439950 | 7.973491 | 7.518824 | 7.638314 | . 3 5.423226 | 6.008737 | 6.766497 | 6.326969 | 7.662835 | 7.150864 | 7.884562 | . 5. Calculate averages for three groups . Then we calculate averages for three groups: . All runners | The Top 10 only | Me only | all_runners = pd.DataFrame(df_min_per_km.mean(), columns=[&quot;all_runners&quot;]) top_10_runners = pd.DataFrame(df_min_per_km.head(10).mean(), columns=[&quot;top_10&quot;]) me = pd.DataFrame(df_min_per_km.filter(items=[&#39;7&#39;], axis=0).mean(), columns=[&quot;me&quot;]) . plots = all_runners.join(top_10_runners, lsuffix = &quot;_all&quot;, rsuffix = &quot;_t10&quot;).join(me) plots.index.rename(&quot;kilometer&quot;, inplace=True) plots . all_runners top_10 me . kilometer . 19.31208 6.284585 | 5.366094 | 5.670050 | . 37.01482 7.424970 | 6.037736 | 6.679095 | . 53.10822 7.954279 | 6.728595 | 7.162089 | . 70.81096 8.467717 | 6.880578 | 7.287312 | . 80.46700 11.937517 | 8.594802 | 9.009009 | . 94.14639 9.395034 | 7.693545 | 7.772254 | . 101.38842 9.952413 | 7.907806 | 7.764890 | . 6. Plot results on line chart . Looking at the results below, we can see that all three groups move in a similar pattern, slowing down substantially at 80KM, before picking up the pace for the final 20KM. . plots.plot(y = [&quot;all_runners&quot;,&quot;top_10&quot;,&quot;me&quot;], figsize=(12,7)) plt.title(&#39;Average pace of runners (minutes per km)&#39;) plt.ylabel(&#39;Minutes per km&#39;, fontsize=&quot;small&quot;) plt.xlabel(&#39;Kilometers&#39;) plt.show() .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/01/02/svp.html",
            "relUrl": "/2018/01/02/svp.html",
            "date": " • Jan 2, 2018"
        }
        
    
  
    
        ,"post7": {
            "title": "Running a Postgres database locally",
            "content": "In this tutorial, I&#39;ll show how you can run a Postgres (also known as PostgreSQL) database locally in docker and connect to it using psql. . Run the Docker container . First we need to run the postgres container: . docker run --name postgres -d -p 5432:5432 -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres -e POSTGRES_DB=employees -v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres . Let&#39;s deconstruct this docker command.. . Run a container called postgres using the postgres image: . docker run --name postgres [OPTIONS] postgres | . Run as a detached container, so it runs in the background of your terminal . -d | . Map port 5432 on the localhost to 5432 in the container . -p 5432:5432 | . Next we pass some Postgres specific environment variables to the Postgres container. You will need to use the User and Password to connect. . -e POSTGRES_PASSWORD=postgres | -e POSTGRES_USER=postgres | -e POSTGRES_DB=employees | . Finally map the container volumne to a local volume: . -v ${PWD}/postgres-docker:/var/lib/postgresql/data postgres | . Connect to the Postgres database . Once the container is up-and-running, we can connect to the Postgres database using the sqlalchemy Python package. First, you create an engine object using the Postgres login credentials specified above: . from sqlalchemy import create_engine db=create_engine(&quot;postgresql://postgres:postgres@localhost:5432/employees&quot;) . Write an SQL string to: . Create a new table called employee_details | Populate this table with some data | bootstrap_sql = &quot;&quot;&quot; CREATE TABLE EMPLOYEE_DETAILS( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL, JOIN_DATE DATE ); INSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE) VALUES (1, &#39;John&#39;, 32, &#39;London&#39;, 20000.00,&#39;2001-07-13&#39;); INSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE) VALUES (2, &#39;David&#39;, 25, &#39;Dublin&#39;, 30000.00, &#39;2007-12-13&#39;); INSERT INTO EMPLOYEE_DETAILS (ID,NAME,AGE,ADDRESS,SALARY,JOIN_DATE) VALUES (3, &#39;Sarah&#39;, 25, &#39;Edinburgh&#39;, 40000.00, &#39;2007-12-13&#39;); &quot;&quot;&quot; . Pass that SQL string to the database engine object: . with db.connect() as con: try: rs = con.execute(bootstrap_sql) ## TODO: Add proper error handling except: pass . View results using psql . First we&#39;ll want to exec into the postgres container: . docker exec -it postgres bash . Once you&#39;re in the container you can run psql commands to query data: . root@12345abcde1:/# root@12345abcde1:/# psql -U postgres -d employees -c &quot;select * from employee_details&quot; id | name | age | address | salary | join_date -+-+--+-+--+ 1 | John | 32 | London | 20000 | 2001-07-13 2 | David | 25 | Dublin | 30000 | 2007-12-13 3 | Sarah | 25 | Edinburgh | 40000 | 2007-12-13 root@12345abcde1:/# .",
            "url": "https://slemasne.github.io/tutorials/tutorials/2018/01/01/run-pg-locally.html",
            "relUrl": "/2018/01/01/run-pg-locally.html",
            "date": " • Jan 1, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi There! I am Stephen Lemasney. I work in FinTech as a Sales Engineer. In this blog I share some Python and data tutorials. .",
          "url": "https://slemasne.github.io/tutorials/tutorials/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://slemasne.github.io/tutorials/tutorials/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}